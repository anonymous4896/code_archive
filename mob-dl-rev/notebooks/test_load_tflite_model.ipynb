{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 13:39:52.184778: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib64/:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/extras/CUPTI/lib64:\n",
      "2022-05-02 13:39:52.184813: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from loguru import logger\n",
    "from src.abs_structure.grpah import Graph\n",
    "from src.abs_structure.node import Node\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "from src.data.make_tflite_graph import load_tflite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 22:23:11.957 | DEBUG    | src.data.make_tflite_graph:load_tflite_model:41 - Processing /opt/Anonymous/workspace/mob-dl-rev/data/raw/tflite_model/dae2a8bbd8aa3cc72aa15d9d15f6b1ac_13729700/tel_recognition_model_4_v3_quant.tflite ...\n",
      "2022-04-30 22:23:11.960 | INFO     | src.data.make_tflite_graph:load_tflite_model:54 - Operator Count: 28\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"../data/raw/tflite_model/com.lanyueming.ps/style_transfer.tflite\"\n",
    "model_path = \"../data/raw/tflite_model/com.yilab.tensorflow.lite.examples.detection/detect.tflite\"\n",
    "model_path = \"/opt/Anonymous/workspace/mob-dl-rev/data/raw/tflite_model/ai.fritz.heartbeat_2019-04-25/hair_a050_224x224_5_1554502837.tflite\"\n",
    "model_path = \"/opt/Anonymous/workspace/mob-dl-rev/data/raw/tflite_model/dae2a8bbd8aa3cc72aa15d9d15f6b1ac_13729700/tel_recognition_model_4_v3_quant.tflite\"\n",
    "# model_path = \"/opt/Anonymous/workspace/mob-dl-rev/data/raw/tflite_model/apk-com.energysh.okcut/model.tflite\"\n",
    "# model_path = \"../data/raw/tflite_model/394a999ea7925d1204c62ec74ddf2805/hed_lite_model_quantize.tflite\"\n",
    "# Load the TFLite model and allocate tensors.\n",
    "g, w = load_tflite_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5641703\n",
      "6745359\n",
      "26667863\n",
      "5641703\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "paths = [\n",
    "# \"apk-com.ghost.sibyl/converted_model_v2_1.tflite\",\n",
    "# \"com.iart.camera.photo_2019-07-10/converted_model_v2_1.tflite\",\n",
    "# \"apk-faceapp.facemystery.learnmoreaboutyourself/converted_model_v2_1.tflite\",\n",
    "# \"wallpaper.transparent/deeplabv3_mnv2_pascal_trainval.tflite\",\n",
    "# \"ru.photostrana.mobile_2019-07-23/model.tflite\",\n",
    "# \"com.lanyueming.ps/lite-model_deeplabv3_1_metadata_2.tflite\",\n",
    "# \"com.lanyueming.ps/deeplabv3_257_mv_gpu.tflite\",\n",
    "# \"com.blink.academy.nomo/deeplabv3_257_mv_gpu.tflite\",\n",
    "# \"8920ba71741fc9002de446b07c1d24bf_46231900/deeplabv3_257_mv_gpu.tflite\",\n",
    "# \"com.jrj.androidtop/model_v13_q.tflite\",\n",
    "# \"com.steam.photoeditor_2019-07-04/converted_model_1.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/hair_a050_224x224_5_1554502837.tflite\",\n",
    "# \"lite_demo_efficientdet2/lite-model_efficientdet_lite2_detection_metadata_1.tflite\",\n",
    "# \"lite_demo_efficientdet2/lite-model_efficientdet_lite0_detection_metadata_1.tflite\",\n",
    "# \"a_i_glass_apkpure.com/detect.tflite\",\n",
    "# \"org.prudhvianddheeraj.lite.example.detection/detect.tflite\",\n",
    "# \"com.infocomltd.ugvassistant_2019-08-05/detect.tflite\",\n",
    "# \"com.yilab.tensorflow.lite.examples.detection/detect.tflite\",\n",
    "# \"com.mm.youliao_47/nsfw.tflite\",\n",
    "# \"com.tencent.ipai_2111111/output_graph.tflite\",\n",
    "# \"com.appsverse.photonapplock/recibos_optimized_graph.tflite\",\n",
    "# \"com.abbyy.mobile.textgrabber.full_258/CropableClassifierV1.tflite\",\n",
    "# \"apk-com.estsoft.picnic/squeeze_sky_net_0104.tflite\",\n",
    "# \"8914ca8f03129b8aca9208bf82bc47c4/detect_model_mobilenetV1_0_5_1280x640.tflite\",\n",
    "# \"com.sogou.translator_101/detect_model_mobilenetV1_0_5_1280x640.tflite\",\n",
    "# \"9c412b795550ac6614184d17ad014fa9_45542300/detect_model_mobilenetV1_0_5_1280x640.tflite\",\n",
    "# \"8914ca8f03129b8aca9208bf82bc47c4/detect_model_mobilenetV1_0_5_1760x800.tflite\",\n",
    "# \"com.sogou.translator_101/detect_model_mobilenetV1_0_5_1760x800.tflite\",\n",
    "# \"9c412b795550ac6614184d17ad014fa9_45542300/detect_model_mobilenetV1_0_5_1760x800.tflite\",\n",
    "# \"net.daum.android.dictionary_2004007/mobilenet_encoder.tflite\",\n",
    "# \"4ca5287b7495786b726a099530d6bd63_62611500/lane_detection_v2.2.0_float32.tflite\",\n",
    "# \"com.sogou.map.android.maps_100907000/lane_detection_v2.2.0_float32.tflite\",\n",
    "# \"975fb88e0abe835264b654adf638b096/lane_detection_v2.2.0_float32.tflite\",\n",
    "# \"4ca5287b7495786b726a099530d6bd63_62611500/lane_detection_v2.2.0_noseg_float32.tflite\",\n",
    "# \"com.sogou.map.android.maps_100907000/lane_detection_v2.2.0_noseg_float32.tflite\",\n",
    "# \"975fb88e0abe835264b654adf638b096/lane_detection_v2.2.0_noseg_float32.tflite\",\n",
    "# \"6b8453244849cec836a149321590d499/object_detect.tflite\",\n",
    "# \"6b8453244849cec836a149321590d499/15_Scene_detection.tflite\",\n",
    "# \"dae2a8bbd8aa3cc72aa15d9d15f6b1ac_13729700/phoneAndBarCodeDetect_2k_202004160957.tflite\",\n",
    "# \"com.kuaidihelp.microbusiness/phoneAndBarCodeDetect_2k_202004160957.tflite\",\n",
    "# \"com.kuaibao.skuaidi_231/phoneAndBarCodeDetect_2k_202004160957.tflite\",\n",
    "# \"dae2a8bbd8aa3cc72aa15d9d15f6b1ac_13729700/tel_recognition_model_4_v3_quant.tflite\",\n",
    "# \"com.kuaidihelp.microbusiness/tel_recognition_model_4_v3_quant.tflite\",\n",
    "# \"com.kuaibao.skuaidi_231/tel_recognition_model_4_v3_quant.tflite\",\n",
    "# \"com.yondor.student_340200/model.tflite\",\n",
    "# \"net.daum.android.dictionary_2004007/mobile-32-B-R-q.tflite\",\n",
    "# \"com.zuoyebang.airclass/person_detect.tflite\",\n",
    "# \"a2e8130e0bec9c72eb82487dbf3e403b_28027500/person_detect.tflite\",\n",
    "# \"com.zuoyebang.airclass/hand_detect_quantized_model.tflite\",\n",
    "# \"a2e8130e0bec9c72eb82487dbf3e403b_28027500/hand_detect_quantized_model.tflite\",\n",
    "# \"com.voltmemo.xz_cidao_350/5000-blackv2-fifty-104x2-plus-long-p96x96-cdraw_model.tflite\",\n",
    "# \"com.voltmemo.xz_cidao_350/5000-colorv4.2-fifty-104x2-p96x96-cdraw_model.tflite\",\n",
    "# \"cn.funtalk.miao_6005/model.tflite\",\n",
    "# \"com.lanyueming.ps/style_predict.tflite\",\n",
    "# \"com.lanyueming.ps/style_transfer.tflite\",\n",
    "# \"com.lanyueming.ps/cartoon_gan_model.tflite\",\n",
    "# \"com.aiworks.android.moji_40104/bbb_quantized.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/poppy_field_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/kaleidoscope_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/starry_night_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/femmes_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/horses_on_seashore_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/bicentennial_print_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/pink_blue_rhombus_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/the_scream_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/head_of_clown_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/the_trial_400x300_025.tflite\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/ritmo_plastico_400x300_025.tflite\",\n",
    "# \"394a999ea7925d1204c62ec74ddf2805/hed_lite_model_quantize.tflite\",\n",
    "# \"3454b466e52a79558c4a48d79eb9f229_53462600/hed_lite_model_quantize.tflite\",\n",
    "# \"460d3d5d1f60c0969cd9441f130c3f44/hed_lite_model_quantize.tflite\",\n",
    "# \"com.ugi.mzkip.ndx/hed_lite_model_quantize.tflite\",\n",
    "# \"com.qihui.elfinbook_126/hed_lite_model_quantize.tflite\",\n",
    "# \"com.jzcfo.jz/hed_lite_model_quantize.tflite\",\n",
    "# \"com.ingbaobei.agent_520/hed_lite_model_quantize.tflite\",\n",
    "# \"com.foxit.mobile.scannedking/hed_lite_model_quantize.tflite\",\n",
    "# \"com.dotc.flashocr/hed_lite_model_quantize.tflite\",\n",
    "# \"com.bjcsxq.chat.carfriend_hj_34/hed_lite_model_quantize.tflite\",\n",
    "# \"com.bangongbao.www/hed_lite_model_quantize.tflite\",\n",
    "# \"com.baasioc.luzhou/hed_lite_model_quantize.tflite\"\n",
    "\"fssd/fssd_25_8bit_gray_v1.tflite\",\n",
    "\"fssd/fssd_25_8bit_v1.tflite\",\n",
    "\"fssd/fssd_100_8bit_gray_v1.tflite\",\n",
    "\"fssd/fssd_25_8bit_gray_v1.tflite\"\n",
    "]\n",
    "\n",
    "base_path = \"../data/raw/tflite_model\"\n",
    "for path in paths:\n",
    "    model_path = \"{}/{}\".format(base_path, path)\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    # print(interpreter._get_ops_details()[27])\n",
    "    # print(list(w[\"{}-287\".format(interpreter.get_tensor_details()[287][\"name\"])]))\n",
    "    # print(interpreter.get_tensor_details()[61])\n",
    "    tmp_count = 0\n",
    "    for tensor_detail in interpreter.get_tensor_details():\n",
    "        tmp_count += (np.prod(tensor_detail[\"shape\"]))\n",
    "    print(tmp_count)\n",
    "# print(interpreter.get_tensor_details()[0])\n",
    "\n",
    "# print(interpreter.get_tensor_details()[23])\n",
    "# print(interpreter.get_tensor_details()[46])\n",
    "# for op in interpreter._get_ops_details():\n",
    "#     if \"DEQUANTIZE\" == op[\"op_name\"]:\n",
    "#         print(op)\n",
    "# print(w[\"strided_slice_4;functional_5/tf_op_layer_strided_slice_4/PartitionedCall/strided_slice_4-23\"])\n",
    "# quantization: -1.6387276649475098 ≤ 0.010796540416777134 * (q - 152) ≤ 1.114390254020691\n",
    "# print((0-152)*0.01079654)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 0, 1)\n",
      "(1, 1, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = tf.constant(np.random.random((1,2,128,2)))\n",
    "print(tf.strided_slice(x, [0,0,0,0], [0,1,0,1], [1,1,1,1], begin_mask=1, end_mask=1).shape)\n",
    "print(x[:,:1,:,1:2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(np.random.random((67, 80)))\n",
    "x = tf.constant(np.random.random((32, 5),dtype=np.int32))\n",
    "# x = x / 255.\n",
    "print(tf.raw_ops.Gather(x, ).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 2, 'op_name': 'DEPTHWISE_CONV_2D', 'inputs': array([27,  3, 33], dtype=int32), 'outputs': array([32], dtype=int32)}\n",
      "{'name': 'MobilenetV1/Conv2d_1_depthwise/depthwise_weights/read', 'index': 3, 'shape': array([1, 3, 3, 8], dtype=int32), 'shape_signature': array([1, 3, 3, 8], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([0.01021576, 0.01179596, 0.00798634, 0.0127729 , 0.01150784,\n",
      "       0.01508466, 0.0100117 , 0.01395334], dtype=float32), 'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), 'quantized_dimension': 3}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "print(interpreter._get_op_details(2))\n",
    "print(interpreter.get_tensor_details()[3])\n",
    "# input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "# interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "# output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "# print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3133621913.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    input_tensor =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "#get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_tensor = np.zeros((224,224,3))\n",
    "#set the tensor to point to the input data to be inferred\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "interpreter.set_tensor(input_index, input_tensor)\n",
    "#Run the inference\n",
    "interpreter.invoke()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8559bc56280362ac4a83fb9144fe0dddf8b8d2e3eef0b8c805268e9624aba74"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mob-dl-rev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
