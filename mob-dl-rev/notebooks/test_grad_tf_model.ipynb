{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 10:07:17.351359: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib64/:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/extras/CUPTI/lib64:\n",
      "2022-04-25 10:07:17.351394: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import easydict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pb(path_to_pb):\n",
    "    \"\"\"Load the .pb file and return graph\n",
    "\n",
    "    :param path_to_pb: The path of the .pb file\n",
    "    :type path_to_pb: str\n",
    "    :return: graph\n",
    "    :rtype: GraphDef\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.gfile.GFile(path_to_pb, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.compat.v1.Graph().as_default() as graph:\n",
    "        tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "        return graph, graph_def\n",
    "\n",
    "def analyze_inputs_outputs(graph):\n",
    "    \"\"\"Analyzing the graph and return the input node and output node\n",
    "\n",
    "    :param graph: The computation graph\n",
    "    :type graph: GraphDef\n",
    "    :return: [Input Node List, Output Node List]\n",
    "    :rtype: List\n",
    "    \"\"\"\n",
    "    ops = graph.get_operations()\n",
    "    outputs_set = set(ops)\n",
    "    inputs = []\n",
    "    for op in ops:\n",
    "        if len(op.inputs) == 0 and op.type != 'Const':\n",
    "            inputs.append(op)\n",
    "        else:\n",
    "            for input_tensor in op.inputs:\n",
    "                if input_tensor.op in outputs_set:\n",
    "                    outputs_set.remove(input_tensor.op)\n",
    "    outputs = list(outputs_set)\n",
    "    return (inputs, outputs)\n",
    "\n",
    "\n",
    "def getRandomImage():\n",
    "    imgDir = \"/opt/Anonymous/workspace/mob-dl-rev/data/external/adv_test_imgs/hotdogs\"\n",
    "    length = len(os.listdir(imgDir))\n",
    "    idx = np.random.randint(low=0, high=length)\n",
    "    img = Image.open(\"{}/{}\".format(imgDir, os.listdir(imgDir)[idx]))\n",
    "    # img.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/opt/Anonymous/workspace/mob-dl-rev/data/raw/tensorflow_model/com.seefoodtechnologies.nothotdog_2017-06-23/deepdog.pb\"\n",
    "graph, graph_def = load_pb(path)\n",
    "input_nodes, output_nodes = analyze_inputs_outputs(graph)\n",
    "print(\"Input Number: {}\".format(len(input_nodes)))\n",
    "print(\"Output Number: {}\".format(len(output_nodes)))\n",
    "for in_node in input_nodes:\n",
    "    print(\"Input Node Name: \" + in_node.name)\n",
    "for out_node in output_nodes:\n",
    "    print(\"Out Node Name: \" + out_node.name)\n",
    "input_tensor = graph.get_tensor_by_name('{}:0'.format(input_nodes[0].name))\n",
    "output_tensor = graph.get_tensor_by_name('{}:0'.format(output_nodes[0].name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "img = getRandomImage()\n",
    "img = img.resize(size=(224, 224))\n",
    "display(img)\n",
    "img = np.asarray(img).astype(np.float32)\n",
    "x = img.reshape(1, 224, 224, 3)\n",
    "x -= 128.\n",
    "x /= 128.\n",
    "print(np.min(x), np.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session(graph=graph)\n",
    "tf.compat.v1.import_graph_def(graph_def)\n",
    "# input_image = np.random.random(size=(1,224,224,3))\n",
    "# target = np.random.random(size=(1,1))\n",
    "# mse_loss = tf.compat.v1.losses.mean_squared_error(output_tensor, target)\n",
    "# grad = tf.compat.v1.gradients(mse_loss, input_tensor)\n",
    "out = sess.run(output_tensor, feed_dict={input_tensor: x})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/Anonymous/anaconda3/envs/mob-dl-rev/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:5214: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "\n",
      "======================End of Report==========================\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "Conv2D                   810.19m float_ops (100.00%, 99.49%)\n",
      "MatMul                   3.04m float_ops (0.51%, 0.37%)\n",
      "Add                      631.05k float_ops (0.14%, 0.08%)\n",
      "MaxPool                  506.88k float_ops (0.06%, 0.06%)\n",
      "Softmax                     60 float_ops (0.00%, 0.00%)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n",
      "Incomplete shape.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "MatMul                   215.84k float_ops (100.00%, 94.46%)\n",
      "Pow                      5.68k float_ops (5.54%, 2.49%)\n",
      "Mul                      3.64k float_ops (3.06%, 1.59%)\n",
      "Add                      2.88k float_ops (1.46%, 1.26%)\n",
      "RealDiv                    142 float_ops (0.20%, 0.06%)\n",
      "Square                     142 float_ops (0.14%, 0.06%)\n",
      "Sum                        141 float_ops (0.08%, 0.06%)\n",
      "Sub                         41 float_ops (0.02%, 0.02%)\n",
      "\n",
      "======================End of Report==========================\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "MatMul                   316.20k float_ops (100.00%, 95.75%)\n",
      "Pow                      6.20k float_ops (4.25%, 1.88%)\n",
      "Mul                      4.17k float_ops (2.37%, 1.26%)\n",
      "Add                      3.15k float_ops (1.11%, 0.95%)\n",
      "RealDiv                    155 float_ops (0.15%, 0.05%)\n",
      "Square                     155 float_ops (0.11%, 0.05%)\n",
      "Sum                        154 float_ops (0.06%, 0.05%)\n",
      "Sub                         41 float_ops (0.01%, 0.01%)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    }
   ],
   "source": [
    "# print(np.sum([np.prod(v.get_shape().as_list()) for v in tf.compat.v1.trainable_variables()]))\n",
    "paths = [\n",
    "# \"com.seefoodtechnologies.nothotdog_2017-06-23/deepdog.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/lv2.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/tz6-s-stitch-sblur-lowtv-gen.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/5-g2-face-fix.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/tz6-s-stitch-blur-gen.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/tz6-s-stitch-gen.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/tensorzoom6-gen.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/tz6-s-stitch-sblur-gen.pb\",\n",
    "# \"uk.tensorzoom_2018-01-18/tz6-l-stitch-gen.pb\",\n",
    "# \"apk-ch.zhaw.facerecognition/optimized_facenet.pb\",\n",
    "# \"com.peace.SilentCamera/people_hq_384x384_1_1543961960_optimized.pb\",\n",
    "# \"demo_ssd_mobilenet_v2/ssd_mobilenet_v2_coco_graph.pb\",\n",
    "# \"demo_ssd_mobilenet_v1_coco/ssd_mobilenet_v1_coco_graph.pb\",\n",
    "# \"demo_mask_rcnn_inception_v2/mask_rcnn_inception_v2_graph.pb\",\n",
    "# \"demo_faster_rcnn_inception_v2_coco/faster_rcnn_inception_v2_coco_graph.pb\",\n",
    "# \"co.vero.app_2019-07-12/frozen_open_nsfw.pb\",\n",
    "# \"com.homesecurity.firstone_2018-06-15/graph.pb\",\n",
    "# \"com.lisa.vibe.camera/cartoon_data.pb\",\n",
    "# \"apk-com.thmobile.sketchphotomaker/stylize_quantized.pb\",\n",
    "# \"pip.face.selfie.beauty.camera.photo.editor_2018-10-29/stylize_quantized.pb\",\n",
    "# \"org.tensorflow.detect_2017-10-09/ssd_mobilenet_v1_android_export.pb\",\n",
    "# \"Object_Detector_v1.2.2_apkpure.com/ssd_mobilenet_v1_coco.pb\",\n",
    "# \"com.app.chakras_opening/ssd_mobilenet_v1_coco.pb\",\n",
    "# \"hien.newsvoicereader/ssd_mobilenet_v1_coco.pb\",\n",
    "# \"org.tensorflow.detect_2017-10-09/tensorflow_inception_graph.pb\",\n",
    "# \"machinelearning.tensorflow.classifier_2017-09-12/tensorflow_inception_graph.pb\",\n",
    "# \"org.tensorflow.detect_2017-10-09/conv_actions_frozen.pb\",\n",
    "# \"Object_Detector_v1.2.2_apkpure.com/mobilenet_frozen.pb\",\n",
    "# \"com.app.chakras_opening/mobilenet_frozen.pb\",\n",
    "# \"hien.newsvoicereader/mobilenet_frozen.pb\",\n",
    "# \"com.tsinghuabigdata.edu.ddmath_554/answer_judge_symbol.pb\",\n",
    "# \"com.steam.photoeditor_2019-07-04/224_1_1_optimized_3.pb\",\n",
    "# \"com.shutterfly_2019-07-10/frozen_model_e.pb\",\n",
    "# \"com.nhn.android.search_2019-09-18/mobilenet_m27.pb\",\n",
    "# \"com.kwalee.drawit_2019-07-19/quickdraw_frozen_long_blacklist_strip_transformed.pb\",\n",
    "# \"cn.runagain.run_80/motionDetecter.pb\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/optimized_inference_graph.pb\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/mobilenet_v2_1_0_224_frozen.pb\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/multi_person_mobilenet_v1_075_float_optimized.pb\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/tile_512x512_025_coherent_optimized.pb\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/bicentennial_print_512x512_025_coherent_optimized.pb\",\n",
    "# \"ai.fritz.heartbeat_2019-04-25/pink_blue_rhombus_512x512_025_coherent_optimized.pb\",\n",
    "# \"apk-faceapp.facemystery.learnmoreaboutyourself/palm_recog_224_1_1.pb\",\n",
    "# \"apk-com.optimaxInvestments.GlassesUSA/mtcnn_freezed_model.pb\"\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/dh9uvyqf.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/abovapkl_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/i7fnh5oq_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/w4jynptk_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/nhd8kloo.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/7amdhcxt.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/jefetsvp_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/uu7sa93u.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/l8vcxbx2.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/art_jp_comic_1.0.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/art_decoration_1.0.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/qiomib2w.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/ocnzc9mp.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/art_tattoo_1.0_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/rnkhybsi.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/sl2umxmv.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/art_picasso_1.0_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/nnamfco3_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/nq257mmc.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/sxnl51wo.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/nobomm6c.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/dnazi3sf.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/spdafynv_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/to94ylj1.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/h2t5d5ib_025.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/h9t4iyhi.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/yvo67bi4.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/xsnzikde.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/oeyvgadt.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/art_us_comic_1.0.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/fcytc5tg.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/wqqapkz3.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/dra0lg9z.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/wlu2atwy.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/rinwzwg2.pb\",\n",
    "# \"com.Video_Mobile_VM.Barnamaj_Damj_Sowar/nus3rdyi_025.pb\",\n",
    "# \"beauty.camera.photo.editor/dh9uvyqf.pb\",\n",
    "# \"beauty.camera.photo.editor/abovapkl_025.pb\",\n",
    "# \"beauty.camera.photo.editor/i7fnh5oq_025.pb\",\n",
    "# \"beauty.camera.photo.editor/w4jynptk_025.pb\",\n",
    "# \"beauty.camera.photo.editor/nhd8kloo.pb\",\n",
    "# \"beauty.camera.photo.editor/7amdhcxt.pb\",\n",
    "# \"beauty.camera.photo.editor/jefetsvp_025.pb\",\n",
    "# \"beauty.camera.photo.editor/uu7sa93u.pb\",\n",
    "# \"beauty.camera.photo.editor/l8vcxbx2.pb\",\n",
    "# \"beauty.camera.photo.editor/art_jp_comic_1.0.pb\",\n",
    "# \"beauty.camera.photo.editor/art_decoration_1.0.pb\",\n",
    "# \"beauty.camera.photo.editor/qiomib2w.pb\",\n",
    "# \"beauty.camera.photo.editor/ocnzc9mp.pb\",\n",
    "# \"beauty.camera.photo.editor/art_tattoo_1.0_025.pb\",\n",
    "# \"beauty.camera.photo.editor/rnkhybsi.pb\",\n",
    "# \"beauty.camera.photo.editor/sl2umxmv.pb\",\n",
    "# \"beauty.camera.photo.editor/art_picasso_1.0_025.pb\",\n",
    "# \"beauty.camera.photo.editor/nnamfco3_025.pb\",\n",
    "# \"beauty.camera.photo.editor/nq257mmc.pb\",\n",
    "# \"beauty.camera.photo.editor/sxnl51wo.pb\",\n",
    "# \"beauty.camera.photo.editor/nobomm6c.pb\",\n",
    "# \"beauty.camera.photo.editor/dnazi3sf.pb\",\n",
    "# \"beauty.camera.photo.editor/spdafynv_025.pb\",\n",
    "# \"beauty.camera.photo.editor/to94ylj1.pb\",\n",
    "# \"beauty.camera.photo.editor/h2t5d5ib_025.pb\",\n",
    "# \"beauty.camera.photo.editor/h9t4iyhi.pb\",\n",
    "# \"beauty.camera.photo.editor/yvo67bi4.pb\",\n",
    "# \"beauty.camera.photo.editor/xsnzikde.pb\",\n",
    "# \"beauty.camera.photo.editor/oeyvgadt.pb\",\n",
    "# \"beauty.camera.photo.editor/art_us_comic_1.0.pb\",\n",
    "# \"beauty.camera.photo.editor/fcytc5tg.pb\",\n",
    "# \"beauty.camera.photo.editor/wqqapkz3.pb\",\n",
    "# \"beauty.camera.photo.editor/dra0lg9z.pb\",\n",
    "# \"beauty.camera.photo.editor/wlu2atwy.pb\",\n",
    "# \"beauty.camera.photo.editor/rinwzwg2.pb\",\n",
    "# \"beauty.camera.photo.editor/nus3rdyi_025.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best1-v2-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/alpha1-v0-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best3-v4-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/alpha3-v0-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/health1-v8-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best2-v3-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best0-v1-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/ware1-v5-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/super1-v6-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/sketch1-v3-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/pencil1-v7-square.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/pencil1-v7-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/alpha1-v0-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/alpha3-v0-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/super1-v6-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/health1-v8-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best3-v4-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best1-v2-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/sketch1-v3-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/ware1-v5-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best0-v1-rect.pb\",\n",
    "# \"apk-com.infixtools.sketchphotomaker.pencil.sketch/best2-v3-rect.pb\",\n",
    "\"com.moment.freedomspeak/frozen_without_dropout.pb\",\n",
    "\"machinelearning.tensorflow.speech_2017-09-26/conv_actions_frozen.pb\",\n",
    "\"apk-com.joytunes.simplypiano/jt_old_model_regression.pb\",\n",
    "\"apk-com.joytunes.simplypiano/jt_extended_model_regression.pb\",\n",
    "]\n",
    "\n",
    "# path = \"apk-com.optimaxInvestments.GlassesUSA/mtcnn_freezed_model.pb\"\n",
    "flops_record = []\n",
    "for path in paths:\n",
    "    base_path = \"/opt/Anonymous/workspace/mob-dl-rev/data/raw/tensorflow_model\"\n",
    "    model_path = \"{}/{}\".format(base_path, path)\n",
    "    graph, graph_def = load_pb(model_path)\n",
    "\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "\n",
    "    # We use the Keras session graph in the call to the profiler.\n",
    "    flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd='op', options=opts)\n",
    "\n",
    "    # return flops.total_float_ops\n",
    "\n",
    "    # flops = tf.compat.v1.profiler.profile(graph, options = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())\n",
    "    # print('FLOP after freezing', flops.total_float_ops)\n",
    "    flops_record.append(flops.total_float_ops)\n",
    "    # typeset = set()\n",
    "    # param_count = 0\n",
    "    # for op in graph.get_operations():\n",
    "    #     typeset.add(op.type)\n",
    "    #     if op.type == \"Const\":\n",
    "    #         param_count += sum([np.product(o.shape) for o in op.outputs])\n",
    "            \n",
    "    # # print(typeset)\n",
    "    # print(param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "814368072\n",
      "228502\n",
      "330227\n"
     ]
    }
   ],
   "source": [
    "for x in flops_record:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.advtool.attack import AdvTool\n",
    "from src.advtool.WhiteBoxAttacks.config import AdvAttackConfig\n",
    "\n",
    "config = AdvAttackConfig()\n",
    "config.clip_range = (-1, 0.9921875)\n",
    "config.eps = 8. / 255. * (config.clip_range[1] - config.clip_range[0])\n",
    "# config.eps = 0.02\n",
    "config.nb_iter = 100\n",
    "pgd_attack = AdvTool.getWhiteBoxAttack(attack_name=\"pgd\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (graph, graph_def, input_tensor, output_tensor)\n",
    "adv_x = pgd_attack.run(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sess.run(output_tensor, feed_dict={input_tensor: adv_x})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.fromarray((adv_x[0]*128 + 128).astype(np.uint8)))\n",
    "# display(Image.fromarray((x[0]*128 + 128).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray((adv_x[0]*128 + 128).astype(np.uint8))\n",
    "img = np.asarray(img).astype(np.float32)\n",
    "x = img.reshape(1, 224, 224, 3)\n",
    "x -= 128.\n",
    "x /= 128.\n",
    "out = sess.run(output_tensor, feed_dict={input_tensor: x})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray((adv_x[0]*128 + 128).astype(np.uint8)).save(\"adv_hotdog_3.bmp\", format=\"bmp\")\n",
    "Image.fromarray((x[0]*128 + 128).astype(np.uint8)).save(\"hotdog_3.bmp\", format=\"bmp\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8559bc56280362ac4a83fb9144fe0dddf8b8d2e3eef0b8c805268e9624aba74"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('mob-dl-rev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
